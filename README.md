# Transformer Implemented with Tensorflow/Keras
## 1. Objective: Implement Transformer from scratch using Tensorflow/Keras
 - Transformer is one of the most critical back bone architecture for SOTA NLP models (BERT, GPT...)
 - Above architectures can be applied not only to NLP data but also to any other sequential data.
 - Rather than using existing libraries such as `torch.nn.Transformer`, `huggingface.transformer`, I decided to implement it from scratch because of two reasons below.
   - It will help me understand transformer much deeper.
   - It has more room to customize so that I can make the model best fit to the situation(or problem).